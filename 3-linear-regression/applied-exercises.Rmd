---
title: Applied Exercises for Chapter 3
author: Jean Cochrane
date: 7 September 2018
---

## Exercise 8

This question involves the use of simple linear regression on the `Auto`
dataset.

### Exercise 8a.

Use the `lm()` function to perform a simple linear regression with `mpg` as the
response and `horsepower` as the predictor. Use the `summary()` function to
print the results. Comment on the output. For example:

i. Is there a relationship between the predictor and the response?
ii. How strong is the relationship between the predictor and the response?
iii. Is the relationship between the predictor and the response positive or
negative?
iv. What is the predicted `mpg` associated with a `horsepower` of 98? What are
the associated 95% confidence and prediction intervals?

### Answer 8a.

Use `lm` to regress `mpg` on `horsepower`:

```{r}
# Load the Auto dataset.
library(ISLR)

# Regress MPG on horsepower.
model = lm(mpg~horsepower, data=Auto)
summary(model)
```

As the results show, there is indeed a relationship between `horsepower` and
`mpg`. The `horsepower` coefficient is around -0.158, meaning that a one-unit
increase in horsepower corresponds to about 0.15 _fewer_ miles per gallon. (This
makes sense, as we would predict that a more powerful engine might be less
efficient.) The relationship is quite significant, with a p-value < 0.001.

Note that while the relationship exists, however, it is not terribly strong. Our
R-squared reports that `horsepower` accounts for only about 60% of the variance of
`mpg`. There are likely other factors with a substantial impact on `mpg`,
although `horsepower` does an OK job approximating the effect.

We can get a predicted confidence interval for a `horsepower` of 98 using the
`predict` method:

```{r}
predict(model, data.frame(horsepower=c(98)), interval="confidence")
```

The model predicts a value for `mpg` of 24.47, with 95% confidence that the true
value lies in the range `[23.97, 24.96]`.

`predict` also allows us to generate a prediction interval:

```{r}
predict(model, data.frame(horsepower=c(98)), interval="prediction")
```

In this case, taking into account an estimate for the irreducible error in the
data, the model predicts a range of `[14.81, 34.12]` -- there's quite a lot of
noise in the data!

## Exercise 8b.

Plot the response and the predictor. Use the `abline()` function to display the
least squares regression line.

## Answer 8b.

```{r}
plot(Auto$horsepower, Auto$mpg)
abline(model, lwd=3, col="red")
```

From this plot, there's a pretty good indication that the true effect is nonlinear.

## Exercise 8c.

Use the `plot()` function to produce diagnostic plots of the least squres
regresion fit. Comment on any problems you see with the fit.

## Answer 8c.

```{r}
par(mfrow=c(2,2))
plot(model)
```

The `Residuals vs Fitted` plot clearly shows that there's a pattern to the
residuals, confirming that the model is not linear.

## Exercise 9

This question involves the use of simple linear regression on the `Auto`
dataset.

### Exercise 9a.

Produce a scatterplot matrix which includes all of the variables in the data
set.

### Answer 9a.

```{r}
pairs(Auto)
```

### Exercise 9b.

Compute the matrix of correlations between the variables using the function
`cor()`. You will need to exclude the `name` variable, which is qualitative.

### Answer 9b.

```{r}
cor(subset(Auto, select=-name))
```

### Exercise 9c.

Use the `lm()` function to perform a multiple linear regression with `mpg` as
the response and all other variables except `name` as the predictors. Use the
`summary()` function to print the results. Comment on the output. For instance:

i. Is there a relationship between the predictors and the response?
ii. Which predictors appear to have a statistically significant relationship to
the response?
iii. What does the coefficient for the `year` variable suggest?

### Answer 9c.

```{r}
lm.fit = lm(mpg~.-name, data=Auto)
summary(lm.fit)
```

i. There is a relationship between the predictors and the response: the
F-statistic renders a p-value very close to 0.
ii. `displacement`, `weight`, `year`, and `origin` all have significant
relationships to the response (`p < 0.05`).
iii. The coefficient for `year` is `~ 0.751`, which indicates that newer cars
have better MPG (and vice versa).

### Exercise 9d.

Use the `plot()` function to produce diagnostic plots of the linear regression
fit. Comment on any problems you see with the fit. Do the residual plots suggest
and unusually large outliers? Does the leverage plot identify any observations
with unusually high leverage?

### Answer 9d.

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

The residuals plot (upper left) indicates a U-shaped pattern in the residuals suggesting
that the relationship may not be linear. In addition, the leverage plot shows
two points with unusually high leverage.

### Exercise 9e.

Use the `*` and `:` symbols to fit linear regression models with interaction
effects. Do any interactions appear to be statistically significant?

### Answer 9e.

Looking at the pairwise correlation matrix produced in answer 9b, a few
correlations are high enough to merit investigating interaction effects:

- `displacement:cylinders`
- `displacement:weight`
- `displacement:horsepower`
- `cylinders:weight`
- `cylinders:horsepower`
- `cylinders:weight`

```{r}
summary(lm(mpg ~ displacement*cylinders, data=Auto))
```

```{r}
summary(lm(mpg ~ displacement*weight, data=Auto))
```

```{r}
summary(lm(mpg ~ displacement*horsepower, data=Auto))
```

```{r}
summary(lm(mpg ~ cylinders*weight, data=Auto))
```

```{r}
summary(lm(mpg ~ cylinders*horsepower, data=Auto))
```

```{r}
summary(lm(mpg ~ horsepower*weight, data=Auto))
```

All of the summaries above indicate statistical significance, but there are only
two interactions that are large enough to be interesting in this domain (>0.001):
`displacement:cylinders` and `cylinders:horsepower`. Try fitting a model
including these two interaction effects:

```{r}
summary(lm(mpg ~ cylinders*displacement + cylinders*horsepower, data=Auto))
```

It appears that `cylinders:horsepower` is the only statistically significant
interaction. However, `cylinders` does not have a significant relationship to
the response in the first place, so the interaction effect is not useful to us.

### Exercise 9f.

Try a few different transformations of the variables, such as `log(X)`, `sqrt(X)`, `X^2`. Comment on your findings.

### Answer 9f.

From the pairwise plots produced in 9a, it appears that `displacement`,
`horsepower`, and `weight` all have plausibly nonlinear relationships to `mpg`.
We'll use `log`, `sqrt`, and the square for each, in no particular order:

```{r}
lm.fit = lm(mpg ~ log(displacement) + sqrt(horsepower) + weight + I(weight^2),
            data=Auto)
summary(lm.fit)
```

According to the summary, `sqrt(horsepower)` and `weight^2` are both
significant, although the effect for `weight` is quite small Let's take a look at the diagnostics:

```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```

The residual plot displays less of a clear pattern, although there are a few potential
outliers, and the variance of the plot is not constant. The leverage plot also
indicates a few high-leverage points. In addition, the Q-Q plot strays from the
expected relationship after the first quantile, indicating that the residuals
are not normally distributed.

Since all three significant variables display relationships to `mpg` that appear
roughly logarithmic, another idea is to try transforming the response instead of
the predictors:

```{r}
lm.fit.log.response = lm(log(mpg) ~ .-name, data=Auto)
summary(lm.fit.log.response)
```

The performance of this model is indeed much better, achieving an R-squared
value of ~0.88. Interestingly, in this model `weight`, `year`, and `origin` are the only significant predictors.
